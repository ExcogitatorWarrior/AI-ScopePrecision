Files are structured in the following way.
1. Each folder contains weights that were trained with the optimizer "optim.Adam(model.parameters(), lr=0.0001) and num_epochs = 100", which means a 0.0001 learning rate for the optimizer provided by PyTorch. These weights have been trained without preloading other weights or any additional pre-training by default. In cases where such default rules are not used, an explanation will be provided in both the name of the folder and the name(s) of the file(s). Folders may contain pictures of losses that are related to the training process and directly affect the training weights, unless specified otherwise. Folders may contain pictures of probes, where the parameters and conditions of taking the probe are equal, with the exception that the weights are not updated. Since the nature of the experiment and the goal of the research are related to a specific way of training, the probe is done on a modified 10k-MNIST dataset. Each picture is copied 9 times, where each copy is shifted down by one pixel from the previous image. This means that the original picture is saved as is, the first copy is shifted down by 1 pixel, the second copy by 2 pixels, and so on, until the ninth picture is shifted down by 9 pixels. It is worth noting that this approach differs from how the training was done. The training was done with batches of 64 images, and therefore the training was done on a slightly different dataset. For each batch of 64 original images, 9 copies were generated, and these batches were placed one after another, shifted down by one pixel. This resulted in a sequence of batches: 64 original images, then a copy of each of these 64 images shifted down by 1 pixel, then another copy of each original image shifted down by 2 pixels, and so on, until the ninth batch of copies, where the original images are shifted down by 9 pixels consistently.
2. The naming convention for folders and weights could and should include certain words to explain how they relate to the project:
	Mode:
		— original or "orig" — mode where scope have been applied to the model during training, so those weights have been trained at each stage of working with copies as input received a 28 leftmost pixels from previous iteration/processing task, until next original image. It also worth to note, that total goal of the model have been modified to grasp precision, which achieved through manipulation with loss function, therefor AI model have never seen other scope than it generated, but potentially could gain understanding of data based on such way of training.
		— no_precision or "no_prec" — mode, where scope have not been applied during training, therefor model have not gained precision feature and there were no attempt to impact loss function. Ultimately basic training, performed for comparison and analytics purpose.
		— cutside — ultimately this is mode where model trained without precision, but it trained to output dark pixels to first pixel of each row in a same way such pixels placed with original precision mode.
	Goal:
		— noised — In this mode AI have received consecutively batches containing 64 images, where next batch up to ninth getting one shift down. At each batch from original to ninth copy main parameter for noise have been multiplied by "counter" variable, AI have been instructed based on loss function to output image containing BOTH shifted image AND noise above it. The noise implemented is salt type of noise.
		— denoise — In this mode AI have received consecutively batches containing 64 images, where next batch up to ninth getting one shift down. At each batch from original to ninth copy main parameter for noise have been multiplied by "counter" variable, AI have been instructed based on loss function to output image containing ONLY shifted image WITHOUT noise above it. The noise implemented is salt type of noise.
		— deadman — In this mode AI have received consecutively batches containing 64 images, where next batch up to ninth getting one shift down. At each batch from original to ninth copy main parameter have been multiplied BY CONSTANT VALUE of 9, AI have been instructed based on loss function to output image containing ONLY shifted image WITHOUT noise above it. The noise implemented is salt type of noise.
	Main Parameter: 
		— Main parameter is value used to generate noise usually placed right after goal identifier in naming of weights or folder. Main parameter may vary from 0.01 up to 0.11, based on nature of the project it does not have sense to put more noise than 0.11, because being multiplied by 9 it would give value of 0.99, which means that 99% of image would be noised, basically at this stage AI would be just guessing, since there couldn't be 101% of image it always would be white screen anyway.
	Comparison:
		— comparison — identifier that following image or folder related to process of comparison for behavior between modes, comparison performed under same conditions, with exception that each mode being set in different kind of situation, same as it were for their mode during training, for example no_precision mode differ from original mode, so each of them processed images under it's own conditions, at other they get same image with absolutely equal noise. 
		— probe — Each probe for comparison contain 10 images, which drafted from "shifted_mnist.bin" dataset, where copies placed right after original image and were shifted in a way described above. I have not been very picky on probes, but i encourage taking your own to prove or disprove my point.
3. Images during comparison structured to contain original image, then input image, one no_precision model receive directly, and original mode receive after scope have been applied, so scoped image displayed after input image, if scope have been applied for this iteration. Right after image with scope there is two outputs which related to weights for comparison, for example there could be "output for mode with precision" and "output for mode without precision", which would mean that first image is original and second is no_precision.
